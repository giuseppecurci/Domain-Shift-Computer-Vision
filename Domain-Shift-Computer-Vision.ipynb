{"cells":[{"cell_type":"markdown","metadata":{"id":"rn1W7CwqyFWZ"},"source":["# Deep Learning - Test-Time Adaptation\n","---\n","###### University of Trento, Academic Year 2023/2024\n","---\n","##### Group 26\n","> <a href=\"https://github.com/giuseppecurci\">Giuseppe Curci</a> \\\n","> 243049\n","\n","> <a href=\"https://github.com/andy295\">Andrea Cristiano</a> \\\n","> 229370\n","---\n","---"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","\n","import torchvision\n","import torchvision.transforms as T\n","import torchvision.models as models\n","from torchvision.transforms import Compose, Normalize, ToTensor\n","\n","import matplotlib\n","from matplotlib import pyplot as plt\n","from matplotlib.lines import Line2D\n","\n","from scipy import stats\n","from scipy.ndimage import zoom\n","\n","from typing import Callable, List, Optional, Tuple\n","from typing import Dict, List\n","\n","from io import BytesIO\n","from pathlib import Path\n","from PIL import Image\n","from tqdm import tqdm\n","from utility.data.get_data import get_data\n","import boto3 # read and write for AWS buckets\n","import clip\n","import cv2\n","import gc\n","import json\n","import math\n","import numpy as np\n","import ollama # if ollama is not available, install by executing the intall_and_run_ollama.sh script\n","import os\n","import random\n","import time\n","import ttach as tta\n","\n","from test_methods.test import Tester\n","\n","from test_time_adaptation.adaptive_bn import adaptive_bn_forward\n","from test_time_adaptation.MEMO import compute_entropy, get_best_augmentations, get_test_augmentations\n","from test_time_adaptation.resnet50_dropout import ResNet50Dropout"],"metadata":{"id":"f686ZK2OOXAc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Introduction\n","\n","(spiegare Domain Shift Test-Time Adaptation cosa siano e perchè siano rilevanti in poche righe)"],"metadata":{"id":"h62t4jTDPk12"}},{"cell_type":"markdown","source":["## Pipeline\n","\n","(idealmente andrebbe creato un disegno che spieghi cosa stiamo facendo, lo possiamo fare anche dopo. Per ora puoi anche limitarti a spiegare a parole oppure lascialo e lo facciamo quando abbiamo il disegno)"],"metadata":{"id":"PicQiiSaPzCV"}},{"cell_type":"markdown","source":["## Utils\n","\n"],"metadata":{"id":"Ge0jVaTAWPYt"}},{"cell_type":"markdown","source":["### File get_data.py"],"metadata":{"id":"qLwYsF2LtEI6"}},{"cell_type":"code","source":["# Class that interacts with images stored in an Amazon S3 bucket.\n","# It allows to load and preprocess images on-the-fly during training or inference.\n","class S3ImageFolder(Dataset):\n","    def __init__(self, root, transform=None):\n","        self.s3_bucket = \"deeplearning2024-datasets\" # name of the bucket\n","        self.s3_region = \"eu-west-1\" # Ireland\n","        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n","        self.transform = transform\n","\n","        # Get list of objects in the bucket\n","        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n","        objects = response.get(\"Contents\", [])\n","        while response.get(\"NextContinuationToken\"):\n","            response = self.s3_client.list_objects_v2(\n","                Bucket=self.s3_bucket,\n","                Prefix=root,\n","                ContinuationToken=response[\"NextContinuationToken\"]\n","            )\n","            objects.extend(response.get(\"Contents\", []))\n","\n","        # Iterate and keep valid files only\n","        self.instances = []\n","        for ds_idx, item in enumerate(objects):\n","            key = item[\"Key\"]\n","            path = Path(key)\n","\n","            # Check if file is valid\n","            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n","                continue\n","\n","            # Get label\n","            label = path.parent.name\n","\n","            # Keep track of valid instances\n","            self.instances.append((label, key))\n","\n","        # Sort classes in alphabetical order (as in ImageFolder)\n","        self.classes = sorted(set(label for label, _ in self.instances))\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","\n","    def __len__(self):\n","        return len(self.instances)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            label, key = self.instances[idx]\n","\n","            # Download image from S3\n","            # response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n","            # img_bytes = response[\"Body\"]._raw_stream.data\n","\n","            img_bytes = BytesIO()\n","            response = self.s3_client.download_fileobj(Bucket=self.s3_bucket, Key=key, Fileobj=img_bytes) # download each image\n","            # img_bytes = response[\"Body\"]._raw_stream.data\n","\n","            # Open image with PIL\n","            img = Image.open(img_bytes).convert(\"RGB\")\n","\n","            # Apply transformations if any\n","            if self.transform is not None:\n","                img = self.transform(img)\n","        except Exception as e:\n","            raise RuntimeError(f\"Error loading image at index {idx}: {str(e)}\")\n","\n","        return img, self.class_to_idx[label]\n","\n","# Function to create DataLoaders for training and evaluating models.\n","# Loads the dataset from the S3 bucket and optionally splits it into training,\n","# validation, and test sets. It then returns PyTorch DataLoader objects for these datasets.\n","def get_data(batch_size, img_root, seed = None, split_data = False, transform = None):\n","\n","    # Load data\n","    data = S3ImageFolder(root=img_root, transform=transform)\n","\n","    if split_data:\n","        # Create train and test splits (80/20)\n","        num_samples = len(data)\n","        training_samples = int(num_samples * 0.8 + 1)\n","        val_samples = int(num_samples * 0.1)\n","        test_samples = num_samples - training_samples - val_samples\n","\n","        torch.manual_seed(seed)\n","        training_data, val_data, test_data = torch.utils.data.random_split(data, [training_samples, val_samples, test_samples])\n","\n","        # Initialize dataloaders\n","        train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=4)\n","        val_loader = torch.utils.data.DataLoader(val_data, batch_size, shuffle=False, num_workers=4)\n","        test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False, num_workers=4)\n","\n","        return train_loader, val_loader, test_loader\n","\n","    data_loader = torch.utils.data.DataLoader(data, batch_size, shuffle=False, num_workers=4)\n","    return data_loader"],"metadata":{"id":"7TDRMeMbWXmt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### File imagenetA_masking.json\n","\n","The **imagenetA_masking.json** file provides a masking for ImageNet-A dataset indices to the standard 1000-class ImageNet output indices used by pre-trained models in PyTorch's torchvision library.\n","\n","Each key in the file corresponds to an index in the standard 1000-class ImageNet output vector. The value associated with each key indicates whether that index should be considered when mapping the 1000-class output to a the smaller set of classes ImageNet-A.\n","\n","A value of -1 indicates that the corresponding index in the 1000-class output should be ignored in the subset of outputs for ImageNet-A.\n","A non-negative integer value indicates that the corresponding index in the 1000-class output should be included in the subset of outputs for ImageNet-A."],"metadata":{"id":"DDAbe13ytTbG"}},{"cell_type":"markdown","source":["### File imagenetA_classes.json\n","\n","The **imagenetA_classes.json** file provides a mapping between the synset IDs used in the ImageNet dataset and their corresponding class names. This mapping is essential for converting model outputs from synset IDs to human-readable labels.\n","\n","The class IDs are mainly used to create the directory structure where newly generated images will be stored, forming a secondary dataset. This dataset can then be utilized for training and inference activities. For more details, refer to the **ToDo** section."],"metadata":{"id":"imfQ4WI6tZQs"}},{"cell_type":"markdown","source":["### File dropout_positions.json\n","\n","The **dropout_positions.json** file is to define the locations within a custom ResNet50 model where dropout layers should be inserted.\n","\n","The dropout layers are incorporated to enhance the proposed method using Monte Carlo Dropout, a technique that improves model robustness and uncertainty estimation. For more details, refer to the **ResNet50Dropout** section."],"metadata":{"id":"pjLXZHpyteaB"}},{"cell_type":"markdown","source":["## Test-time adaptation methods"],"metadata":{"id":"FXKWLOkhWn1N"}},{"cell_type":"markdown","source":["### MEMO: Test Time Robustness via Adaptation and Augmentation<sup>[4]</sup>\n","\n","In this paper, the authors propose a method called MEMO (Marginal Entropy Minimization with One Test Point) designed to address the problem of robustness in deep neural networks when confronted with distribution shifts or unexpected perturbations. To achieve this, the method employs both adaptation and augmentation strategies.\n","\n","![Figure 1](\\images\\MEMO.png)\n","\n","Unlike traditional approaches that focus on modifying the training process, MEMO utilizes the information provided by test inputs. It applies data augmentations to a single test input to generate various versions of the input, and from these, it calculates the marginal output distribution. The model's parameters are then updated to minimize the entropy of this marginal distribution. Finally, the model uses these updated parameters to make a prediction on the original test input.\n","\n","\n","\n","This approach allows MEMO to enhance the model's robustness against unseen distribution changes by effectively adapting to new conditions during testing."],"metadata":{"id":"hWtwo8X73663"}},{"cell_type":"markdown","source":["### TTA - Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation<sup>[5]</sup>\n","\n","In this paper, the authors propose a method called Greedy Policy Search (GPS) for learning test-time data augmentation policies that enhance the performance of machine learning models. The key idea is that data augmentation policies, typically designed for the training phase, can also be learned and optimized during the test phase to improve model performance.\n","\n","![Figure 2](\\images\\) TODO ADD image\n","\n","The method involves iteratively selecting sub-policies that maximize a chosen performance criterion, such as the calibrated log-likelihood on a validation set. As the name suggests, GPS constructs the augmentation policy in a greedy, step-by-step manner, ensuring that each added sub-policy contributes to performance improvement.\n","\n","This approach offers a simple yet powerful baseline for learning test-time augmentation policies, providing a promising alternative to more complex methods like reinforcement learning or Bayesian optimization, which are traditionally used for training-phase augmentation."],"metadata":{"id":"gG6WmGER3-wT"}},{"cell_type":"markdown","source":["### Adaptive Batch Normalization - Improving robustness against common corruptions by covariate shift adaptation<sup>[6]<sup>"],"metadata":{"id":"1HVxP1v04Bhm"}},{"cell_type":"markdown","source":["### Monte Carlo Dropout\n","\n","Monte Carlo Dropout (MC Dropout) is a technique that leverages dropout, a regularization method commonly used during training, to also perform approximate Bayesian inference during the testing phase.\n","\n","During the model training phase, the dropout technique randomly \"drops\" or deactivates a fraction of neurons in the network during each forward pass. The probability of dropping neurons can be controlled using a specific parameter. The aim is to prevent neurons from memorizing specific inputs, thus reducing overfitting and encouraging the network to learn more general representations.\n","\n","However, during the model test phase, dropout is usually turned off, allowing the full network to make predictions. The key idea behind Monte Carlo Dropout is to keep dropout active during the test phase and perform multiple forward passes through the network. The result is that for each forward pass, a different dropout mask is used, which means a random subset of neurons is activated, allowing for different predictions. By averaging these predictions, it is possible to obtain both the final prediction and a measure of the model's uncertainty.\n","\n","The technique was originally introduced by Yarin Gal and Zoubin Ghahramani in their seminal paper, \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\"[2]."],"metadata":{"id":"Hlq8QGqJ6byL"}},{"cell_type":"code","source":["class ResNet50Dropout(nn.Module):\n","\"\"\"\n","It creates a version of the ResNet-50 model that integrates dropout layers at\n","various points in the architecture. By using dropout, the model can be trained\n","with a regularization technique that allows for the implementation of\n","Monte Carlo Dropout.\n","----------\n","weights: Optional pre-trained weights for the ResNet-50 model.\n","dropout_rate: The probability of dropping out neurons during training.\n","A value of 0 means no dropout is applied and the architecture is identical to the\n","original ResNet-50.\n","\"\"\"\n","    def __init__(self, weights=None, dropout_rate=0.):\n","        super(ResNet50Dropout, self).__init__()\n","\n","        self.weights = weights\n","        self.model = models.resnet50(weights=self.weights)\n","        self.dropout_rate = dropout_rate\n","\n","        self.dropout_positions = []\n","        if self.dropout_rate > 0:\n","            self.dropout_positions = self.get_dropout_positions()\n","\n","        self._add_dropout()\n","\n","    # This method reads a JSON file that contains a list of layer names where\n","    # dropout should be applied.\n","    def get_dropout_positions(self):\n","        dropout_positions_path = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/utility/data/dropout_positions.json\"\n","        with open(dropout_positions_path, 'r') as json_file:\n","            dropout_positions = json.load(json_file)\n","        dropout_positions = dropout_positions[\"dropout_positions\"]\n","\n","        return dropout_positions\n","\n","    # This method adds dropout layers to the ResNet-50 model at the specified\n","    # positions, by looking at the dropout_positions list.\n","    # For each specified layer, the method wraps the original layer in\n","    # a nn.Sequential block, which includes the original layer followed by a\n","    # nn.Dropout layer with the specified dropout rate.\n","    def _add_dropout(self):\n","        if 'conv1' in self.dropout_positions:\n","            self.model.conv1 = nn.Sequential(\n","                self.model.conv1,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer1' in self.dropout_positions:\n","            self.model.layer1 = nn.Sequential(\n","                self.model.layer1,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer2' in self.dropout_positions:\n","            self.model.layer2 = nn.Sequential(\n","                self.model.layer2,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer3' in self.dropout_positions:\n","            self.model.layer3 = nn.Sequential(\n","                self.model.layer3,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer4' in self.dropout_positions:\n","            self.model.layer4 = nn.Sequential(\n","                self.model.layer4,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'avgpool' in self.dropout_positions:\n","            self.model.avgpool = nn.Sequential(\n","                self.model.avgpool,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'fc' in self.dropout_positions:\n","            self.model.fc = nn.Sequential(\n","                nn.Dropout(p=self.dropout_rate),\n","                self.model.fc\n","            )\n","\n","    def forward(self, x):\n","        return self.model(x)"],"metadata":{"id":"diLFBJgTXA8d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DiffTPT - Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning<sup>[7]</sup>\n"],"metadata":{"id":"6X8HL-vDoMRf"}},{"cell_type":"code","source":["# difftpt and image generation code"],"metadata":{"id":"9EbufrbhXD1H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["—-- our proposal MEMO + stable diffusion + LLM"],"metadata":{"id":"37p1q69DXIrN"}},{"cell_type":"code","source":["def get_imagenetA_classes():\n","    \"\"\"\n","    ImageNet-A uses the same label structure as the original ImageNet (ImageNet-1K).\n","    Each class in ImageNet is represented by a synset ID (e.g., n01440764 for \"tench, Tinca tinca\").\n","    This function returns a dictionary that maps the synset IDs of ImageNet-A to the corresponding class names.\n","    ----------\n","    indices_in_1k: list of indices to map [B,1000] -> [B,200]\n","    \"\"\"\n","    imagenetA_classes_path = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/utility/data/imagenetA_classes.json\"\n","    imagenetA_classes_dict = None\n","    with open(imagenetA_classes_path, 'r') as json_file:\n","        imagenetA_classes_dict = json.load(json_file)\n","\n","    # Ensure `class_dict` is a dictionary with keys as class IDs and values as class names\n","    class_dict = {k: v for k, v in imagenetA_classes_dict.items()}\n","    return class_dict\n","\n","def create_dir_generated_images(path):\n","    classes = list(get_imagenetA_classes().values())\n","    for class_name in classes:\n","        class_path = os.path.join(path, class_name)\n","        os.makedirs(class_path, exist_ok=True)"],"metadata":{"id":"ithq1PDCXO6R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### File install_and_run_ollama.sh\n","\n","To create the new images, we decided to use a StableDiffusion model to which we provided a list of prompts as input, one for each image to be generated. To generate the prompts, we used an LLM, specifically Llama 3.1. We were able to obtain this model through the Ollama library[3], which provides the model and all the necessary configurations for its use. To get everything needed to download and run Ollama, and therefore Llama 3.1, simply execute the file **install_and_run_ollama.sh**. The script will download and install Ollama, start it, and instantiate Llama 3.1."],"metadata":{"id":"BVdw53L-qQzS"}},{"cell_type":"markdown","source":["### File llm_context.json\n","\n","The **llm_context.json** file is used to provide context for generating prompts for a text-to-image generator model.\n","\n","Each entry in the file specifies a Role and Content:\n","\n","* **Role**: Specifies who is speaking or interacting in the conversation. It can either be \"system\" (representing the LLM) or \"user\" (representing the person providing input to the LLM).\n","* **Content**: This field contains the actual message or instructions being communicated. It includes system instructions or user-provided input regarding the prompts to be generated.\n","\n","The messages can be of the following types:\n","1.    A message that sets the system's role and context, instructing the LLM on how to generate prompts for a text-to-image generation task.\n","2.    A message that serves as an example input a user might provide. It helps demonstrate how the user specifies the class name, number of prompts, and style of the picture.\n","3    A message that provides an example output from the LLM, demonstrating the kind of response it should generate based on the provided input.\n","\n","In conclusion, the content of the file provides a framework that guides the LLM in understanding the context of the conversation, adhering to rules, and producing the desired output format."],"metadata":{"id":"NnC1hwRQwWEj"}},{"cell_type":"markdown","source":["## Image generation"],"metadata":{"id":"UadfMJj_y5pC"}},{"cell_type":"code","source":["—-- image_generator.py (TODO)"],"metadata":{"id":"9ovsvQOCXUjx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing\n","\n","The Tester class is designed to facilitate the running of experiments involving a deep neural network model. It provides methods to manage various aspects of the experimental setup, including configuring models and optimizers, handling augmentations, computing statistics, and saving results."],"metadata":{"id":"7gcvLUxbXa8-"}},{"cell_type":"code","source":["class Tester:\n","    \"\"\"\n","    A class to run all the experiments. It stores all the informations to reproduce the experiments in a json file\n","    at exp_path.\n","    \"\"\"\n","    def __init__(self, model, optimizer, exp_path, device):\n","        self.__model = model\n","        self.__optimizer = optimizer\n","        self.__device = device\n","        self.__exp_path = exp_path\n","\n","    def save_result(self, accuracy, path_result, num_augmentations, augmentations, seed_augmentations, top_augmentations, MEMO, num_adaptation_steps, lr_setting, weights, prior_strength, time_test, use_MC):\n","        \"\"\"\n","        Takes all information of the experiment saves it in a json file stored at exp_path\n","        \"\"\"\n","        data = {\n","            \"accuracy\": accuracy,\n","            \"top_augmentations\" : top_augmentations,\n","            \"use_MEMO\" : MEMO,\n","            \"num_adaptation_steps\" : num_adaptation_steps,\n","            \"lr_setting\" : lr_setting,\n","            \"weights\" : weights,\n","            \"num_augmentations\" : num_augmentations,\n","            \"seed_augmentations\": seed_augmentations,\n","            \"augmentations\" : [str(augmentation) for augmentation in augmentations],\n","            \"prior_strength\" : prior_strength,\n","            \"MC\" : use_MC,\n","            \"time_test\" : time_test\n","        }\n","        try:\n","            with open(path_result, 'w') as json_file:\n","                json.dump(data, json_file)\n","        except:\n","            print(\"Result were not saved\")\n","\n","    def get_model(self, weights_imagenet, MC):\n","        \"\"\"\n","        Utility function to instantiate a torch model. The argument weights_imagenet should have\n","        a value in accordance with the parameter weights of torchvision.models.\n","        \"\"\"\n","        if MC:\n","            self.__model=ResNet50Dropout(weights=weights_imagenet, dropout_rate=MC['dropout_rate'])\n","            model = self.__model\n","        else:\n","            model = self.__model(weights=weights_imagenet)\n","\n","        model.to(self.__device)\n","        model.eval()\n","        return model\n","\n","    def get_optimizer(self, model, lr_setting:list):\n","        \"\"\"\n","        Utility function to instantiate a torch optimizer.\n","        ----------\n","        lr_setting: must be a list containing either one global lr for the whole model or a dictionary\n","        where each value is a list with a list of parameters' names and a lr for those parameters.\n","        e.g.\n","        lr_setting = [{\n","            \"classifier\" : [[\"fc.weight\", \"fc.bias\"], 0.00025]\n","            }, 0]\n","        lr_setting = [0.00025]\n","        \"\"\"\n","        if len(lr_setting) == 2:\n","            layers_groups = []\n","            lr_optimizer = []\n","            for layers, lr_param_name in lr_setting[0].items():\n","                layers_groups.extend(lr_param_name[0])\n","                params = [param for name, param in model.named_parameters() if name in lr_param_name[0]]\n","                lr_optimizer.append({\"params\":params, \"lr\": lr_param_name[1]})\n","            other_params = [param for name, param in model.named_parameters() if name not in layers_groups]\n","            lr_optimizer.append({\"params\":other_params})\n","            optimizer = self.__optimizer(lr_optimizer, lr = lr_setting[1], weight_decay = 0)\n","        else:\n","            optimizer = self.__optimizer(model.parameters(), lr = lr_setting[0], weight_decay = 0)\n","        return optimizer\n","\n","    def get_imagenetA_masking(self):\n","        \"\"\"\n","        All torchvision models output a tensor [B,1000] with \"B\" being the batch dimension. This function\n","        returns a list of indices to apply to the model's output to use the model on imagenet-A dataset.\n","        ----------\n","        indices_in_1k: list of indices to map [B,1000] -> [B,200]\n","        \"\"\"\n","        imagenetA_masking_path = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/utility/data/imagenetA_masking.json\"\n","        with open(imagenetA_masking_path, 'r') as json_file:\n","            imagenetA_masking = json.load(json_file)\n","        indices_in_1k = [int(k) for k in imagenetA_masking if imagenetA_masking[k] != -1]\n","        return indices_in_1k\n","\n","    def get_monte_carlo_statistics(self, mc_logits):\n","        \"\"\"\n","        Compute mean, median, mode and standard deviation of the Monte Carlo samples.\n","        \"\"\"\n","        statistics = {}\n","        mean_logits = mc_logits.mean(dim=0)\n","        statistics['mean'] = mean_logits\n","\n","        median_logits = mc_logits.median(dim=0).values\n","        statistics['median'] = median_logits\n","\n","        pred_classes = mc_logits.argmax(dim=1)\n","        pred_classes_cpu = pred_classes.cpu().numpy()\n","        mode_predictions, _ = stats.mode(pred_classes_cpu, axis=0)\n","        mode_predictions = torch.tensor(mode_predictions.squeeze(), dtype=torch.long)\n","        statistics['mode'] = mode_predictions\n","\n","        uncertainty = mc_logits.var(dim=0)\n","        statistics['std'] = uncertainty\n","        return statistics\n","\n","    def get_prediction(self, image_tensors, model, masking, TTA = False, top_augmentations = 0, MC = None):\n","        \"\"\"\n","        Takes a tensor of images and outputs a prediction for each image.\n","        ----------\n","        image_tensors: is a tensor of [B,C,H,W] if TTA is used or if both MEMO and TTA are not used, or of dimension [C,H,W]\n","                       if only MEMO is used\n","        masking: a list of indices to map the imagenet1k logits to the one of imagenet-A\n","        top_augmentations: a non-negative integer, if greater than 0 then the \"top_augmentations\" with the lowest entropy are\n","                           selected to make the final prediction\n","        MC: a dictionary containing the number of evaluations using Monte Carlo Dropout and the dropout rate\n","        \"\"\"\n","        if MC:\n","            model.train()  # enable dropout by setting the model to training mode\n","            mc_logits = []\n","            for _ in range(MC['num_samples']):\n","                logits = model(image_tensors)[:,masking] if image_tensors.dim() == 4 else model(image_tensors.unsqueeze(0))[:,masking]\n","                mc_logits.append(logits)\n","            mc_logits = torch.stack(mc_logits, dim=0)\n","            if TTA:\n","                # first mean is over MC samples, second mean is over TTA augmentations\n","                probab_augmentations = F.softmax(mc_logits - mc_logits.max(dim=2, keepdim=True)[0], dim=2)\n","                if top_augmentations:\n","                    probab_augmentations = self.get_best_augmentations(probab_augmentations, top_augmentations)\n","                y_pred = probab_augmentations.mean(dim=0).mean(dim=0).argmax().item()\n","                statistics = self.get_monte_carlo_statistics(probab_augmentations.mean(dim=1))\n","                return y_pred, statistics\n","            statistics = self.get_monte_carlo_statistics(mc_logits)\n","            return statistics['median'].argmax(dim=1), statistics\n","        else:\n","            logits = model(image_tensors)[:,masking] if image_tensors.dim() == 4 else model(image_tensors.unsqueeze(0))[:,masking]\n","            if TTA:\n","                probab_augmentations = F.softmax(logits - logits.max(dim=1)[0][:, None], dim=1)\n","                if top_augmentations:\n","                    probab_augmentations = self.get_best_augmentations(probab_augmentations, top_augmentations)\n","                y_pred = probab_augmentations.mean(dim=0).argmax().item()\n","                return y_pred, None\n","            return logits.argmax(dim=1), None\n","\n","    def compute_entropy(self, probabilities: torch.tensor):\n","        \"\"\"\n","        See MEMO.py\n","        \"\"\"\n","        return compute_entropy(probabilities)\n","\n","    def get_best_augmentations(self, probabilities: torch.tensor, top_k: int):\n","        \"\"\"\n","        See MEMO.py\n","        \"\"\"\n","        return get_best_augmentations(probabilities, top_k)\n","\n","    def get_test_augmentations(self, input:torch.tensor, augmentations:list, num_augmentations:int, seed_augmentations:int):\n","        \"\"\"\n","        See MEMO.py\n","        \"\"\"\n","        return get_test_augmentations(input, augmentations, num_augmentations, seed_augmentations)\n","\n","    def retrieve_synthetic_images(self):\n","        \"\"\"\n","        Function to retrieve the synthetically generated images before test time using CLIP embeddings.\n","        \"\"\"\n","        pass\n","\n","    def test(self,\n","             augmentations:list,\n","             num_augmentations:int,\n","             seed_augmentations:int,\n","             img_root:str,\n","             lr_setting:list,\n","             weights_imagenet = None,\n","             dataset = \"imagenetA\",\n","             batch_size = 64,\n","             MEMO = False,\n","             num_adaptation_steps = 0,\n","             top_augmentations = 0,\n","             TTA = False,\n","             prior_strength = -1,\n","             verbose = True,\n","             log_interval = 1,\n","             MC = None):\n","        \"\"\"\n","        Main function to test a torchvision model with different test-time adaptation techniques\n","        and keep track of the results and the experiment setting.\n","        ---\n","        augmentations: list of torchvision.transforms functions.\n","        num_augmentations: the number of augmentations to use for each sample to perform test-time adaptation.\n","        seed_augmentations: seed to reproduce the sampling of augmentations.\n","        img_root: str path to get a dataset in a torch format.\n","        lr_setting: list with lr instructions to adapt the model. See \"get_optimizer\" for more details.\n","        weights_imagenet: weights_imagenet should have a value in accordance with the parameter\n","                          weights of torchvision.models.\n","        dataset: the name of the dataset to use. Note: this parameter doesn't directly control the data\n","                 used, it's only used to use the right masking to map the models' outputs to the right dimensions.\n","                 At the moment only Imagenet-A masking is supported.\n","        MEMO: a boolean to use marginal entropy minimization with one test point\n","        TTA: a boolean to use test time augmentation\n","        top_augmentations: if MEMO or TTA are set to True, then values higher than zero select the top_augmentations\n","                           with the lowest entropy (highest confidence).\n","        prior_strength: defines the weight given to pre-trained statistics in BN adaptation. If negative, then no BN\n","                        adaptation is applied.\n","        verbose: use loading bar to visualize accuracy and number of batch during testing.\n","        log_interval: defines after how many batches a new accuracy should be displayed. Default is 1, thus\n","                      after each batch a new value is displayed.\n","        num_adaptation_steps: (TODO)\n","        MC: dictionary containing the number of evaluations using Monte Carlo Dropout and the dropout rate.\n","        \"\"\"\n","        # check some basic conditions\n","        assert bool(num_adaptation_steps) == MEMO, \"When using MEMO adaptation steps should be > 1, otherwise equal to 0.\"\n","        if not (MEMO or TTA):\n","            assert not (num_augmentations or top_augmentations), \"If both MEMO and TTA are set to False, then top_augmentations and num_augmentations must be 0\"\n","        assert not lr_setting if not MEMO else True, \"If MEMO is false, then lr_setting must be None\"\n","        assert isinstance(prior_strength, (float,int)) , \"Prior adaptation must be either a float or an int\"\n","\n","        # get the name of the weigths used and define the name of the experiment\n","        weights_name = str(weights_imagenet).split(\".\")[-1] if weights_imagenet else \"MEMO_repo\"\n","        use_MC = True if MC else False\n","        name_result = f\"MEMO_{MEMO}_AdaptSteps_{num_adaptation_steps}_adaptBN_{prior_strength}_TTA_{TTA}_aug_{num_augmentations}_topaug_{top_augmentations}_seed_aug_{seed_augmentations}_weights_{weights_name}_MC_{use_MC}\"\n","        path_result = os.path.join(self.__exp_path,name_result)\n","        assert not os.path.exists(path_result),f\"MEMO test already exists: {path_result}\"\n","\n","        # in case of using dropout, check if the model is a ResNet50Dropout and the parameters are correct\n","        if MC:\n","            assert isinstance(self.__model, ResNet50Dropout), f\"To use dropout the model must be a ResNet50Dropout\"\n","            assert MC['num_samples'] > 1, f\"To use dropout the number of samples must be greater than 1\"\n","\n","        # transformation pipeline used in ResNet-50 original training\n","        transform_loader = T.Compose([\n","            T.Resize(256),\n","            T.CenterCrop(224),\n","            T.ToTensor()\n","        ])\n","\n","        # to use after model's update\n","        normalize_input = T.Compose([\n","                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","                    ])\n","\n","        test_loader = get_data(batch_size, img_root, transform = transform_loader, split_data=False)\n","        model = self.get_model(weights_imagenet, MC)\n","\n","        # if MEMO is used, create a checkpoint to reload after each model and optimizer update\n","        if MEMO:\n","            optimizer = self.get_optimizer(model = model, lr_setting = lr_setting)\n","            MEMO_checkpoint_path = os.path.join(self.__exp_path,\"checkpoint.pth\")\n","            torch.save({\n","                'model': model.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, MEMO_checkpoint_path)\n","            MEMO_checkpoint = torch.load(MEMO_checkpoint_path)\n","\n","        if dataset == \"imagenetA\":\n","            imagenetA_masking = self.get_imagenetA_masking()\n","\n","        if prior_strength < 0:\n","            torch.nn.BatchNorm2d.prior_strength = 1\n","        else:\n","            torch.nn.BatchNorm2d.prior_strength = prior_strength / (prior_strength + 1)\n","            torch.nn.BatchNorm2d.forward = adaptive_bn_forward\n","\n","        # Initialize a dictionary to store accumulated time for each step\n","        time_dict = {\n","            \"MEMO_update\": 0.0,\n","            \"get_augmentations\": 0.0,\n","            \"confidence_selection\": 0.0,\n","            \"get_prediction\": 0.0,\n","            \"total_time\": 0.0\n","        }\n","\n","        samples = 0.0\n","        cumulative_accuracy = 0.0\n","\n","        for batch_idx, (inputs, targets) in enumerate(test_loader):\n","            inputs, targets = inputs.to(self.__device), targets.to(self.__device)\n","            if MEMO or TTA:\n","                for input, target in zip(inputs, targets):\n","                    if MEMO:\n","                        model.load_state_dict(MEMO_checkpoint['model'])\n","                        model.eval()\n","                        optimizer.load_state_dict(MEMO_checkpoint['optimizer'])\n","\n","                    # get normalized augmentations\n","                    start_time_augmentations = time.time()\n","                    test_augmentations = self.get_test_augmentations(input, augmentations, num_augmentations, seed_augmentations)\n","                    end_time_augmentations = time.time()\n","                    time_dict[\"get_augmentations\"] += (end_time_augmentations - start_time_augmentations)\n","\n","                    test_augmentations = test_augmentations.to(self.__device)\n","                    for _ in range(num_adaptation_steps):\n","                        logits = model(test_augmentations)\n","\n","                        # apply imagenetA masking\n","                        if dataset == \"imagenetA\":\n","                            logits = logits[:, imagenetA_masking]\n","                        # compute stable softmax\n","                        probab_augmentations = F.softmax(logits - logits.max(dim=1)[0][:, None], dim=1)\n","\n","                        # confidence selection for augmentations\n","                        if top_augmentations:\n","                            start_time_confidence_selection = time.time()\n","                            probab_augmentations = self.get_best_augmentations(probab_augmentations, top_augmentations)\n","                            end_time_confidence_selection = time.time()\n","                            time_dict[\"confidence_selection\"] += (end_time_confidence_selection - start_time_confidence_selection)\n","\n","                        if MEMO:\n","                            start_time_memo_update = time.time()\n","                            marginal_output_distribution = torch.mean(probab_augmentations, dim=0)\n","                            marginal_loss = self.compute_entropy(marginal_output_distribution)\n","                            marginal_loss.backward()\n","                            optimizer.step()\n","                            optimizer.zero_grad()\n","                            end_time_memo_update = time.time()\n","                            time_dict[\"MEMO_update\"] += (end_time_memo_update - start_time_memo_update)\n","\n","                    start_time_prediction = time.time()\n","                    with torch.no_grad():\n","                        if TTA:\n","                            # statistics:\n","                            # dictionary containing statistics resulting from the application of monte carlo dropout\n","                            # look at get_monte_carlo_statistics() for more details\n","                            y_pred, statistics = self.get_prediction(test_augmentations, model, imagenetA_masking, TTA, top_augmentations, MC=MC)\n","                        else:\n","                            input = normalize_input(input)\n","                            y_pred, statistics = self.get_prediction(input, model, imagenetA_masking, MC=MC)\n","                        cumulative_accuracy += int(target == y_pred)\n","                    end_time_prediction = time.time()\n","                    time_dict[\"get_prediction\"] += (end_time_prediction - start_time_prediction)\n","            else:\n","                start_time_prediction = time.time()\n","                with torch.no_grad():\n","                    inputs = normalize_input(inputs)\n","                    y_pred = self.get_prediction(inputs, model, imagenetA_masking, MC=MC)\n","\n","                    # Handle cases where targets or y_pred might be tuples\n","                    if isinstance(targets, tuple):\n","                        targets = targets[0]  # Extract the relevant tensor\n","                    if isinstance(y_pred, tuple):\n","                        y_pred = y_pred[0]  # Extract the relevant tensor\n","\n","                    if targets.dim() == 0 or y_pred.dim() == 0:\n","                        # If both targets and y_pred are scalars\n","                        correct_predictions = int(targets == y_pred)\n","                    else:\n","                        # If targets and y_pred are tensors\n","                        correct_predictions = (targets == y_pred).sum().item()\n","\n","                cumulative_accuracy += correct_predictions\n","\n","                end_time_prediction = time.time()\n","                time_dict[\"get_prediction\"] += (end_time_prediction - start_time_prediction)\n","\n","            samples += inputs.shape[0]\n","\n","            if verbose and batch_idx % log_interval == 0:\n","                current_accuracy = cumulative_accuracy / samples * 100\n","                print(f'Batch {batch_idx}/{len(test_loader)}, Accuracy: {current_accuracy:.2f}%', end='\\r')\n","\n","        accuracy = cumulative_accuracy / samples * 100\n","        time_dict[\"total_time\"] += sum(time_dict.values())\n","\n","        self.save_result(accuracy = accuracy,\n","                         path_result = path_result,\n","                         seed_augmentations = seed_augmentations,\n","                         num_augmentations = num_augmentations,\n","                         augmentations = augmentations,\n","                         top_augmentations = top_augmentations,\n","                         MEMO = MEMO,\n","                         num_adaptation_steps = num_adaptation_steps,\n","                         lr_setting = lr_setting,\n","                         weights = weights_name,\n","                         prior_strength = prior_strength,\n","                         use_MC = use_MC,\n","                         time_test = time_dict)\n","\n","        return accuracy"],"metadata":{"id":"sGbfzoA-Xjah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["- Experiments (qui metti solo le celle di codice da runnare per riprodurre i risultati):\n","—- resnet50 + SGD (lr, momentum, weight_decay)\n","—- resnet50 + ADAM (lr, momentum, weight_decay)"],"metadata":{"id":"kbV1_cGTXoJI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Results\n","\n","<table>\n","  <thead>\n","    <tr>\n","      <th rowspan=\"1\">Experiment</th>\n","      <th rowspan=\"1\">Dataset</th>\n","      <th rowspan=\"1\">Base Model</th>\n","      <th rowspan=\"1\">Weights</th>\n","      <th rowspan=\"1\", colspan=\"2\">Optimizer</th>\n","      <th rowspan=\"1\">Optimization Steps</th>\n","      <th rowspan=\"1\", colspan=\"3\">Augmentations</th>\n","      <th rowspan=\"1\">Batch Size</th>\n","      <th rowspan=\"1\">MEMO</th>\n","      <th rowspan=\"1\">Confidence Selection</th>\n","      <th rowspan=\"1\", colspan=\"2\">BN</th>\n","      <th rowspan=\"1\">TTA</th>\n","      <th rowspan=\"1\", colspan=\"3\">MC</th>\n","      <th rowspan=\"1\">Accuracy</th>\n","      <th rowspan=\"1\">Execution Time</th>\n","    </tr>\n","    <tr>\n","      <th>Nr.</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>Type</th>\n","      <th>LR</th>\n","      <th>Nr.</th>\n","      <th>Type</th>\n","      <th>Number</th>\n","      <th>Seed</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>Prior Strength</th>\n","      <th></th>\n","      <th></th>\n","      <th>Dropout rate</th>\n","      <th>Nr. Samples</th>\n","      <th>%</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody align=\"center\">\n","    <tr>\n","      <th>1</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>-</td>\n","      <td>1</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.026</td>\n","      <td>00:00:20</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>-</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.253</td>\n","      <td>00:26:20</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>-</td>\n","      <td>1</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>-</td>\n","      <td>1</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.026</td>\n","      <td>00:00:23</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.16</td>\n","      <td>00:33:20</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.853</td>\n","      <td>00:38:44</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.213</td>\n","      <td>00:42:03</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.853</td>\n","      <td>00:47:37</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.213</td>\n","      <td>00:41:19</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.826</td>\n","      <td>00:46:55</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td></td>\n","      <td></td>\n","    </tr>  \n","  </tbody>\n","</table>\n"],"metadata":{"id":"zwzAwcHxXtKG"}},{"cell_type":"markdown","source":["## Discussion\n","\n","Discussione dei risultati"],"metadata":{"id":"J67VmQSzXvXe"}},{"cell_type":"markdown","source":["## Conclusion\n","\n","wrap up di quello fatto e possibili alternative per future sperimentazioni"],"metadata":{"id":"9BY3pvNJXwzd"}},{"cell_type":"markdown","source":["## Bibliography\n","\n","1. **Schneider, Steffen and Rusak, Evgenia and Eck, Luisa and Bringmann, Oliver and Brendel, Wieland and Bethge, Matthias.** \"Improving robustness against common corruptions by covariate shift adaptation.\" Advances in Neural Information Processing Systems, Vol. 33, 2020, pp. 11539-11551. [https://proceedings.neurips.cc/paper_files/paper/2020/file/85690f81aadc1749175c187784afc9ee-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/85690f81aadc1749175c187784afc9ee-Paper.pdf).\n","\n","2. **Gal, Yarin and Ghahramani, Zoubin** \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.\" Proceedings of The 33rd International Conference on Machine Learning, Vol. 48, 2016, pp. 1050-1059. [https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html).\n","\n","3. [Ollama library](https://ollama.com/)\n","\n","4. **Marvin Zhang and Sergey Levine and Chelsea Finn.** \"MEMO: Test Time Robustness via Adaptation and Augmentation.\" Advances in neural information processing systems, Vol. 35, 2021, pp. 38629-38642. [https://arxiv.org/abs/2110.09506](https://arxiv.org/abs/2110.09506).\n","\n","5. **Lyzhov, Alexander and Molchanova, Yuliya and Ashukha, Arsenii and Molchanov, Dmitry and Vetrov, Dmitry.** \"Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation.\" Proceedings of Machine Learning Research, Vol. 124, 2020, pp. 1308-1317. [https://proceedings.mlr.press/v124/lyzhov20a.html](https://proceedings.mlr.press/v124/lyzhov20a.html).\n","\n","6. **Schneider, Steffen and Rusak, Evgenia and Eck, Luisa and Bringmann, Oliver and Brendel, Wieland and Bethge, Matthias.** \"Improving robustness against common corruptions by covariate shift adaptation.\" Advances in Neural Information Processing Systems, Vol. 33, 2020, pp. 11539-11551. [https://proceedings.neurips.cc/paper_files/paper/2020/file/85690f81aadc1749175c187784afc9ee-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/85690f81aadc1749175c187784afc9ee-Paper.pdf).\n","\n","7. **Feng, Chun-Mei and Yu, Kai and Liu, Yong and Khan, Salman and Zuo, Wangmeng.** \"Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning.\" 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 2704-2714. [https://arxiv.org/abs/2308.06038](https://arxiv.org/abs/2308.06038).\n","\n","\n"],"metadata":{"id":"vkWfJE_fXhvw"}}],"metadata":{"colab":{"collapsed_sections":["Ge0jVaTAWPYt","qLwYsF2LtEI6","Hlq8QGqJ6byL","6X8HL-vDoMRf","UadfMJj_y5pC","7gcvLUxbXa8-"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}